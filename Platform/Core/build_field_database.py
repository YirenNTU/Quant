#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
ğŸ“Š Field Database Builder - æ¬„ä½è³‡æ–™åº«å»ºæ§‹å™¨
================================================================================

å°‡ã€ŒæŒ‰å…¬å¸åˆ†é¡ã€çš„ JSON è³‡æ–™è½‰æ›æˆã€ŒæŒ‰æ¬„ä½åˆ†é¡ã€çš„è³‡æ–™åº«ã€‚

ã€è½‰æ›å‰ã€‘Stock_Pool/Database/
â”œâ”€â”€ 1101_20260206.json  â†’ å°æ³¥çš„æ‰€æœ‰è³‡æ–™
â”œâ”€â”€ 2330_20260206.json  â†’ å°ç©é›»çš„æ‰€æœ‰è³‡æ–™
â””â”€â”€ ...

ã€è½‰æ›å¾Œã€‘Platform/FieldDB/
â”œâ”€â”€ price/
â”‚   â”œâ”€â”€ open.parquet    â†’ æ‰€æœ‰å…¬å¸çš„ Open (rows=æ—¥æœŸ, cols=è‚¡ç¥¨ä»£ç¢¼)
â”‚   â”œâ”€â”€ close.parquet   â†’ æ‰€æœ‰å…¬å¸çš„ Close
â”‚   â”œâ”€â”€ volume.parquet  â†’ æ‰€æœ‰å…¬å¸çš„ Volume
â”‚   â””â”€â”€ ...
â”œâ”€â”€ financials/
â”‚   â”œâ”€â”€ tej_gpm.parquet â†’ æ‰€æœ‰å…¬å¸çš„æ¯›åˆ©ç‡ (rows=å­£åº¦, cols=è‚¡ç¥¨ä»£ç¢¼)
â”‚   â”œâ”€â”€ tej_opm.parquet â†’ æ‰€æœ‰å…¬å¸çš„ç‡Ÿç›Šç‡
â”‚   â””â”€â”€ ...
â”œâ”€â”€ chip/
â”‚   â”œâ”€â”€ qfii_ex.parquet â†’ æ‰€æœ‰å…¬å¸çš„å¤–è³‡è²·è³£è¶…
â”‚   â””â”€â”€ ...
â”œâ”€â”€ monthly_sales/
â”‚   â”œâ”€â”€ d0003.parquet   â†’ æ‰€æœ‰å…¬å¸çš„æœˆç‡Ÿæ”¶ YoY
â”‚   â””â”€â”€ ...
â””â”€â”€ _meta/
    â”œâ”€â”€ tickers.json    â†’ è‚¡ç¥¨ä»£ç¢¼æ¸…å–®
    â”œâ”€â”€ field_map.json  â†’ æ¬„ä½å°ç…§è¡¨
    â””â”€â”€ build_info.json â†’ å»ºæ§‹è³‡è¨Š

ã€ä½¿ç”¨æ–¹å¼ã€‘
>>> from Platform.Core.field_db import FieldDB
>>> db = FieldDB()
>>> df = db.get('close')  # å–å¾—æ‰€æœ‰å…¬å¸æ”¶ç›¤åƒ¹
>>> df['2330']            # å°ç©é›»æ”¶ç›¤åƒ¹

Author: Investment AI Platform
Version: 1.0
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from io import StringIO
from glob import glob
from typing import Dict, List, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# è¨­å®š
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# è·¯å¾‘è¨­å®š
SCRIPT_DIR = Path(__file__).parent
PLATFORM_DIR = SCRIPT_DIR.parent
PROJECT_ROOT = PLATFORM_DIR.parent

# ä¾†æºèˆ‡ç›®æ¨™
SOURCE_DIR = PROJECT_ROOT / "Stock_Pool" / "Database"
OUTPUT_DIR = PLATFORM_DIR / "FieldDB"

# è¼¸å‡ºæ ¼å¼ (parquet æ›´å¿«æ›´å°, csv æ›´é€šç”¨)
OUTPUT_FORMAT = "parquet"  # "parquet" or "csv"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ¬„ä½å®šç¾© - å®šç¾©è¦æå–çš„æ¬„ä½
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

FIELD_DEFINITIONS = {
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PRICE - è‚¡åƒ¹è³‡æ–™ (æ—¥é »ï¼Œç´„ 485 å¤©)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ä¾†æº: data_downloader.py â†’ loader.get_history(ticker, period_days=730)
    # API: TWN/APIPRCD
    # æ ¼å¼: orient='split', index=æ—¥æœŸ, columns=æ¬„ä½
    # å®Œæ•´åº¦: 99%+ (æ‰€æœ‰æ¬„ä½éƒ½æœ‰è³‡æ–™)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "price": {
        "source_key": "price",
        "date_column": None,  # ä½¿ç”¨ DataFrame index (å·²ç¶“æ˜¯æ—¥æœŸ)
        "fields": {
            # åƒ¹æ ¼
            "open": {"column": "Open", "description": "é–‹ç›¤åƒ¹"},
            "high": {"column": "High", "description": "æœ€é«˜åƒ¹"},
            "low": {"column": "Low", "description": "æœ€ä½åƒ¹"},
            "close": {"column": "Close", "description": "æ”¶ç›¤åƒ¹"},
            "adjfac": {"column": "adjfac", "description": "é‚„åŸå› å­"},
            
            # æˆäº¤
            "volume": {"column": "Volume", "description": "æˆäº¤é‡(è‚¡)"},
            "amount": {"column": "amt", "description": "æˆäº¤é‡‘é¡"},
            "trades": {"column": "trn", "description": "æˆäº¤ç­†æ•¸"},
            "avgprc": {"column": "avgprc", "description": "å‡åƒ¹"},
            "turnover": {"column": "turnover", "description": "é€±è½‰ç‡%"},
            
            # å¸‚å ´
            "mktcap": {"column": "mktcap", "description": "å¸‚å€¼"},
            "shares": {"column": "shares", "description": "æµé€šè‚¡æ•¸"},
            
            # ä¼°å€¼
            "pe": {"column": "per", "description": "æœ¬ç›Šæ¯”"},
            "pb": {"column": "pbr", "description": "è‚¡åƒ¹æ·¨å€¼æ¯”"},
            "psr": {"column": "psr_tej", "description": "è‚¡åƒ¹ç‡Ÿæ”¶æ¯”"},
            "pe_tej": {"column": "per_tej", "description": "PE(TEJ)"},
            "pb_tej": {"column": "pbr_tej", "description": "PB(TEJ)"},
            
            # æ®–åˆ©ç‡
            "div_yield": {"column": "div_yid", "description": "æ®–åˆ©ç‡%"},
            "cdiv_yield": {"column": "cdiv_yid", "description": "ç¾é‡‘æ®–åˆ©ç‡%"},
            
            # å ±é…¬
            "daily_return": {"column": "roi", "description": "æ—¥å ±é…¬ç‡%"},
            "amplitude": {"column": "hmlpct", "description": "æŒ¯å¹…%"},
        }
    },
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # FINANCIALS - æç›Šè¡¨ (å­£é »ï¼Œç´„ 20 å­£)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ä¾†æº: data_downloader.py â†’ loader.get_financials(ticker, quarters=8)
    # API: TWN/AINVFINB
    # æ ¼å¼: orient='split', index=ç§‘ç›®åç¨±, columns=æ—¥æœŸ
    # éœ€è¦è½‰ç½®: row=ç§‘ç›® â†’ row=æ—¥æœŸ
    # 
    # âš ï¸ TEJ åˆå…¥æ±Ÿæ¹–ç‰ˆé™åˆ¶ï¼š
    # - æ²’æœ‰ Operating Income, EBIT, Pretax Income ç­‰ç´°é …
    # - ä½†æœ‰ TEJ è¨ˆç®—å¥½çš„æ¯”ç‡ (GPM, OPM, é€±è½‰ç‡ç­‰)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "financials": {
        "source_key": "financials",
        "date_column": None,
        "transpose": True,
        "fields": {
            # æç›Šé …ç›® (æœ‰è³‡æ–™)
            "revenue": {"column": "Total Revenue", "description": "ç‡Ÿæ¥­æ”¶å…¥"},
            "gross_profit": {"column": "Gross Profit", "description": "æ¯›åˆ©"},
            "net_income": {"column": "Net Income", "description": "ç¨…å¾Œæ·¨åˆ©"},
            
            # TEJ è¨ˆç®—çš„æ¯”ç‡ (æœ‰è³‡æ–™)
            "tej_gpm": {"column": "TEJ_GPM", "description": "æ¯›åˆ©ç‡%"},
            "tej_opm": {"column": "TEJ_OPM", "description": "ç‡Ÿç›Šç‡%"},
            
            # é€±è½‰ç‡æŒ‡æ¨™ (æœ‰è³‡æ–™)
            "inventory_turnover": {"column": "Inventory Turnover", "description": "å­˜è²¨é€±è½‰ç‡"},
            "inventory_days": {"column": "Inventory Days", "description": "å­˜è²¨å¤©æ•¸"},
            "dso": {"column": "Days Sales Outstanding", "description": "æ‡‰æ”¶å¸³æ¬¾å¤©æ•¸"},
            "days_payable": {"column": "Days Payable", "description": "æ‡‰ä»˜å¸³æ¬¾å¤©æ•¸"},
        }
    },
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # BALANCE_SHEET - è³‡ç”¢è² å‚µè¡¨ (å­£é »ï¼Œç´„ 20 å­£)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ä¾†æº: data_downloader.py â†’ loader.get_financials(ticker, quarters=8)
    # API: TWN/AINVFINB
    # 
    # âš ï¸ TEJ åˆå…¥æ±Ÿæ¹–ç‰ˆé™åˆ¶ï¼š
    # - åªæœ‰å½™ç¸½æ•¸å­— (Total Assets, Total Debt ç­‰)
    # - æ²’æœ‰ç´°é … (Inventory, Cash, Current Liabilities ç­‰)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "balance_sheet": {
        "source_key": "balance_sheet",
        "date_column": None,
        "transpose": True,
        "fields": {
            # æœ‰è³‡æ–™çš„æ¬„ä½
            "total_assets": {"column": "Total Assets", "description": "è³‡ç”¢ç¸½é¡"},
            "total_debt": {"column": "Total Debt", "description": "è² å‚µç¸½é¡"},
            "total_liabilities": {"column": "Total Liabilities Net Minority Interest", "description": "ç¸½è² å‚µ"},
            "current_assets": {"column": "Current Assets", "description": "æµå‹•è³‡ç”¢"},
            "accounts_receivable": {"column": "Accounts Receivable", "description": "æ‡‰æ”¶å¸³æ¬¾"},
            
            # ä»¥ä¸‹æ¬„ä½ TEJ åˆå…¥æ±Ÿæ¹–ç‰ˆç„¡è³‡æ–™ï¼Œä½†ä¿ç•™å®šç¾©ä»¥å‚™å‡ç´š
            # "inventory": {"column": "Inventory", "description": "å­˜è²¨"},
            # "cash": {"column": "Cash And Cash Equivalents", "description": "ç¾é‡‘"},
            # "current_liabilities": {"column": "Current Liabilities", "description": "æµå‹•è² å‚µ"},
            # "long_term_debt": {"column": "Long Term Debt", "description": "é•·æœŸè² å‚µ"},
        }
    },
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CASHFLOW - ç¾é‡‘æµé‡è¡¨ (å­£é »ï¼Œç´„ 20 å­£)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ä¾†æº: data_downloader.py â†’ loader.get_financials(ticker, quarters=8)
    # API: TWN/AINVFINB
    # 
    # âš ï¸ TEJ åˆå…¥æ±Ÿæ¹–ç‰ˆé™åˆ¶ï¼š
    # - åªæœ‰ OCF (ç‡Ÿæ¥­ç¾é‡‘æµ)
    # - æ²’æœ‰ ICF, FCF, CAPEX
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "cashflow": {
        "source_key": "cashflow",
        "date_column": None,
        "transpose": True,
        "fields": {
            # æœ‰è³‡æ–™çš„æ¬„ä½
            "ocf": {"column": "Operating Cash Flow", "description": "ç‡Ÿæ¥­ç¾é‡‘æµ"},
            
            # ä»¥ä¸‹æ¬„ä½ TEJ åˆå…¥æ±Ÿæ¹–ç‰ˆç„¡è³‡æ–™ï¼Œä½†ä¿ç•™å®šç¾©ä»¥å‚™å‡ç´š
            # "icf": {"column": "Investing Cash Flow", "description": "æŠ•è³‡ç¾é‡‘æµ"},
            # "fcf": {"column": "Financing Cash Flow", "description": "ç±Œè³‡ç¾é‡‘æµ"},
            # "capex": {"column": "Capital Expenditure", "description": "è³‡æœ¬æ”¯å‡º"},
        }
    },
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CHIP - ç±Œç¢¼è³‡æ–™ (æ—¥é »ï¼Œç´„ 42 å¤©)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ä¾†æº: data_downloader.py â†’ download_chip_data(ticker, days=60)
    # API: TWN/APISHRACT
    # æ ¼å¼: orient='split', index=row number, columns=æ¬„ä½ (å« mdate)
    # å®Œæ•´åº¦: 100% (æ‰€æœ‰æ¬„ä½éƒ½æœ‰è³‡æ–™)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "chip": {
        "source_key": "chip",
        "date_column": "mdate",
        "fields": {
            # æ³•äººè²·è³£è¶…
            "qfii_net": {"column": "qfii_ex", "description": "å¤–è³‡è²·è³£è¶…(å¼µ)"},
            "fund_net": {"column": "fund_ex", "description": "æŠ•ä¿¡è²·è³£è¶…(å¼µ)"},
            "dealer_net": {"column": "tot_ex", "description": "ä¸‰å¤§æ³•äººåˆè¨ˆ"},
            
            # æ³•äººæŒè‚¡æ¯”ä¾‹
            "qfii_pct": {"column": "qfii_pct", "description": "å¤–è³‡æŒè‚¡%"},
            "fund_pct": {"column": "fd_pct", "description": "æŠ•ä¿¡æŒè‚¡%"},
            "dealer_pct": {"column": "dlr_pct", "description": "è‡ªç‡Ÿå•†æŒè‚¡%"},
            
            # èè³‡èåˆ¸
            "margin_long": {"column": "long_t", "description": "èè³‡é¤˜é¡(å¼µ)"},
            "margin_short": {"column": "short_t", "description": "èåˆ¸é¤˜é¡(å¼µ)"},
            "short_ratio": {"column": "s_l_pct", "description": "åˆ¸è³‡æ¯”%"},
        }
    },
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MONTHLY_SALES - æœˆç‡Ÿæ”¶ (æœˆé »ï¼Œç´„ 15 å€‹æœˆ)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ä¾†æº: data_downloader.py â†’ download_monthly_sales(ticker, months=15)
    # API: TWN/APISALE
    # æ ¼å¼: orient='split', index=row number, columns=æ¬„ä½ (å« mdate)
    # å®Œæ•´åº¦: 94%+ (æ‰€æœ‰æ¬„ä½éƒ½æœ‰è³‡æ–™)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "monthly_sales": {
        "source_key": "monthly_sales",
        "date_column": "mdate",
        "fields": {
            "monthly_rev": {"column": "d0001", "description": "ç•¶æœˆç‡Ÿæ”¶(åƒå…ƒ)"},
            "monthly_rev_alt": {"column": "d0002", "description": "æœˆç‡Ÿæ”¶(åƒå…ƒ)"},
            "monthly_rev_yoy": {"column": "d0003", "description": "æœˆç‡Ÿæ”¶YoY%"},
            "monthly_rev_mom": {"column": "d0004", "description": "æœˆç‡Ÿæ”¶MoM%"},
            "ytd_rev": {"column": "d0005", "description": "ç´¯è¨ˆç‡Ÿæ”¶(åƒå…ƒ)"},
            "ytd_rev_yoy": {"column": "d0006", "description": "ç´¯è¨ˆç‡Ÿæ”¶YoY%"},
            "ytd_rev_yoy_pct": {"column": "d0007", "description": "ç´¯è¨ˆç‡Ÿæ”¶MoM%"},
        }
    },
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # DIVIDEND - è‚¡åˆ©è³‡æ–™ (ğŸ†• æ–°å¢)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ä¾†æº: data_downloader.py â†’ download_dividend_data(ticker, years=5)
    # API: TWN/APIDV1
    # æ ¼å¼: orient='split', index=row number, columns=æ¬„ä½ (å« mdate)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "dividend": {
        "source_key": "dividend",
        "date_column": "mdate",
        "fields": {
            "cash_div": {"column": "divc", "description": "ç¾é‡‘è‚¡åˆ©"},
            "stock_div": {"column": "divs", "description": "è‚¡ç¥¨è‚¡åˆ©"},
            "div_type": {"column": "distri_type", "description": "é…æ¯é¡å‹"},
            "ex_div_date": {"column": "edexdate", "description": "é™¤æ¯æ—¥"},
            "pay_date": {"column": "div_date", "description": "ç™¼æ”¾æ—¥"},
        }
    },
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SELF_ANNOUNCED - è‡ªçµæ•¸ (ğŸ†• æ–°å¢)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ä¾†æº: data_downloader.py â†’ download_self_announced(ticker, months=24)
    # API: TWN/AFESTM1
    # æ ¼å¼: orient='split', index=row number, columns=æ¬„ä½ (å« mdate)
    # 
    # è‡ªçµæ•¸æ¯”å­£å ±æ›´å³æ™‚ï¼Œå…¬å¸è‡ªè¡Œå…¬å¸ƒçš„è²¡å‹™æ•¸æ“š
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "self_announced": {
        "source_key": "self_announced",
        "date_column": "mdate",
        "fields": {
            "sa_revenue": {"column": "ip12", "description": "è‡ªçµç‡Ÿæ”¶"},
            "sa_opi": {"column": "opi", "description": "è‡ªçµç‡Ÿæ¥­åˆ©ç›Š"},
            "sa_pretax": {"column": "isibt", "description": "è‡ªçµç¨…å‰æ·¨åˆ©"},
            "sa_net_income": {"column": "isnip", "description": "è‡ªçµç¨…å¾Œæ·¨åˆ©"},
            "sa_eps": {"column": "eps", "description": "è‡ªçµEPS"},
            "sa_gpm": {"column": "r105", "description": "è‡ªçµæ¯›åˆ©ç‡%"},
            "sa_opm": {"column": "r106", "description": "è‡ªçµç‡Ÿç›Šç‡%"},
            "sa_npm": {"column": "r107", "description": "è‡ªçµæ·¨åˆ©ç‡%"},
            "sa_rev_yoy": {"column": "r401", "description": "è‡ªçµç‡Ÿæ”¶æˆé•·ç‡%"},
            "sa_opi_yoy": {"column": "r403", "description": "è‡ªçµç‡Ÿæ¥­åˆ©ç›Šæˆé•·ç‡%"},
            "sa_ni_yoy": {"column": "r404", "description": "è‡ªçµæ·¨åˆ©æˆé•·ç‡%"},
        }
    },
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CAPITAL - è³‡æœ¬å½¢æˆ (ğŸ†• æ–°å¢)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ä¾†æº: data_downloader.py â†’ download_capital_change(ticker, years=3)
    # API: TWN/APISTK1
    # æ ¼å¼: orient='split', index=row number, columns=æ¬„ä½ (å« mdate)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "capital": {
        "source_key": "capital",
        "date_column": "mdate",
        "fields": {
            "capital_amt": {"column": "stk_amt", "description": "è‚¡æœ¬(åƒå…ƒ)"},
            "shares_outstanding": {"column": "slamt", "description": "æµé€šè‚¡æ•¸(åƒè‚¡)"},
            "cash_increase": {"column": "cash", "description": "ç¾é‡‘å¢è³‡"},
            "earning_increase": {"column": "earning", "description": "ç›ˆé¤˜è½‰å¢è³‡"},
            "capital_reserve": {"column": "capital", "description": "è³‡æœ¬å…¬ç©"},
            "employee_bonus": {"column": "bonus", "description": "å“¡å·¥ç´…åˆ©"},
            "capital_decrease": {"column": "cap_dec", "description": "æ¸›è³‡"},
        }
    },
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CHIP_EXTENDED - ç±Œç¢¼è³‡æ–™æ“´å…… (ğŸ†• æ–°å¢æ›´å¤šæ¬„ä½)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ä¾†æº: data_downloader.py â†’ download_chip_data (æ“´å……ç‰ˆ)
    # API: TWN/APISHRACT
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "chip_extended": {
        "source_key": "chip",
        "date_column": "mdate",
        "fields": {
            # å¤–è³‡è²·/è³£é‡
            "qfii_buy": {"column": "qfii_buy", "description": "å¤–è³‡è²·é€²é‡(å¼µ)"},
            "qfii_sell": {"column": "qfii_sell", "description": "å¤–è³‡è³£å‡ºé‡(å¼µ)"},
            # æŠ•ä¿¡è²·/è³£é‡
            "fund_buy": {"column": "fund_buy", "description": "æŠ•ä¿¡è²·é€²é‡(å¼µ)"},
            "fund_sell": {"column": "fund_sell", "description": "æŠ•ä¿¡è³£å‡ºé‡(å¼µ)"},
            # ç¶­æŒç‡
            "margin_maintenance": {"column": "lmr", "description": "èè³‡ç¶­æŒç‡%"},
            "short_maintenance": {"column": "smr", "description": "èåˆ¸ç¶­æŒç‡%"},
            "total_maintenance": {"column": "tmr", "description": "æ•´æˆ¶ç¶­æŒç‡%"},
            # å€Ÿåˆ¸
            "stock_lending": {"column": "borr_t1", "description": "å€Ÿåˆ¸é¤˜é¡(å¼µ)"},
        }
    },
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¸»ç¨‹å¼
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class FieldDatabaseBuilder:
    """æ¬„ä½è³‡æ–™åº«å»ºæ§‹å™¨"""
    
    def __init__(self, source_dir: Path = SOURCE_DIR, output_dir: Path = OUTPUT_DIR):
        self.source_dir = Path(source_dir)
        self.output_dir = Path(output_dir)
        self.tickers = []
        self.ticker_names = {}
        self.field_map = {}
        self.stats = {
            "total_files": 0,
            "success_files": 0,
            "failed_files": 0,
            "total_fields": 0,
            "build_time": None,
        }
    
    def build(self):
        """åŸ·è¡Œå®Œæ•´å»ºæ§‹æµç¨‹"""
        start_time = datetime.now()
        
        print("=" * 70)
        print("ğŸ“Š Field Database Builder - æ¬„ä½è³‡æ–™åº«å»ºæ§‹å™¨")
        print("=" * 70)
        print(f"ä¾†æºç›®éŒ„: {self.source_dir}")
        print(f"è¼¸å‡ºç›®éŒ„: {self.output_dir}")
        print(f"è¼¸å‡ºæ ¼å¼: {OUTPUT_FORMAT}")
        print("=" * 70)
        
        # Step 1: æƒæä¾†æºæª”æ¡ˆ
        print("\nğŸ“‚ Step 1: æƒæä¾†æºæª”æ¡ˆ...")
        source_files = self._scan_source_files()
        if not source_files:
            print("âŒ æ‰¾ä¸åˆ°ä¾†æºæª”æ¡ˆï¼")
            return False
        
        # Step 2: è¼‰å…¥æ‰€æœ‰å…¬å¸è³‡æ–™
        print(f"\nğŸ“¥ Step 2: è¼‰å…¥ {len(source_files)} å®¶å…¬å¸è³‡æ–™...")
        all_data = self._load_all_data(source_files)
        
        # Step 3: å»ºç«‹è¼¸å‡ºç›®éŒ„
        print("\nğŸ“ Step 3: å»ºç«‹è¼¸å‡ºç›®éŒ„çµæ§‹...")
        self._create_output_dirs()
        
        # Step 4: ä¾æ¬„ä½é¡åˆ¥è™•ç†
        print("\nğŸ”„ Step 4: è½‰æ›è³‡æ–™...")
        for category, config in FIELD_DEFINITIONS.items():
            self._process_category(category, config, all_data)
        
        # Step 5: å„²å­˜ metadata
        print("\nğŸ’¾ Step 5: å„²å­˜ metadata...")
        self._save_metadata()
        
        # å®Œæˆ
        self.stats["build_time"] = str(datetime.now() - start_time)
        
        print("\n" + "=" * 70)
        print("âœ… å»ºæ§‹å®Œæˆï¼")
        print("=" * 70)
        self._print_summary()
        
        return True
    
    def _scan_source_files(self) -> List[Path]:
        """æƒæä¾†æº JSON æª”æ¡ˆ"""
        pattern = str(self.source_dir / "*.json")
        files = glob(pattern)
        
        # éæ¿¾ä¸¦å–å¾—æœ€æ–°ç‰ˆæœ¬
        ticker_files = {}
        for f in files:
            filename = os.path.basename(f)
            parts = filename.replace('.json', '').split('_')
            if len(parts) >= 2:
                ticker = parts[0]
                date = parts[1]
                
                # ä¿ç•™æœ€æ–°æ—¥æœŸçš„æª”æ¡ˆ
                if ticker not in ticker_files or date > ticker_files[ticker][1]:
                    ticker_files[ticker] = (f, date)
        
        result = [Path(v[0]) for v in ticker_files.values()]
        print(f"   æ‰¾åˆ° {len(result)} å®¶å…¬å¸è³‡æ–™")
        self.stats["total_files"] = len(result)
        
        return sorted(result)
    
    def _load_all_data(self, files: List[Path]) -> Dict[str, dict]:
        """è¼‰å…¥æ‰€æœ‰å…¬å¸è³‡æ–™"""
        all_data = {}
        
        for i, file_path in enumerate(files):
            ticker = file_path.stem.split('_')[0]
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                all_data[ticker] = data
                self.tickers.append(ticker)
                
                # è¨˜éŒ„å…¬å¸åç¨±
                if data.get('info'):
                    self.ticker_names[ticker] = data['info'].get('shortName', ticker)
                
                self.stats["success_files"] += 1
                
            except Exception as e:
                print(f"   âš ï¸ è¼‰å…¥å¤±æ•— {ticker}: {e}")
                self.stats["failed_files"] += 1
            
            # é€²åº¦é¡¯ç¤º
            if (i + 1) % 50 == 0 or (i + 1) == len(files):
                print(f"   é€²åº¦: {i+1}/{len(files)} ({(i+1)/len(files)*100:.1f}%)")
        
        return all_data
    
    def _create_output_dirs(self):
        """å»ºç«‹è¼¸å‡ºç›®éŒ„çµæ§‹"""
        # ä¸»ç›®éŒ„
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # å„é¡åˆ¥å­ç›®éŒ„
        for category in FIELD_DEFINITIONS.keys():
            (self.output_dir / category).mkdir(exist_ok=True)
        
        # metadata ç›®éŒ„
        (self.output_dir / "_meta").mkdir(exist_ok=True)
        
        print(f"   å»ºç«‹ç›®éŒ„: {self.output_dir}")
    
    def _process_category(self, category: str, config: dict, all_data: Dict[str, dict]):
        """è™•ç†ä¸€å€‹è³‡æ–™é¡åˆ¥"""
        print(f"\n   ğŸ“Š {category.upper()}")
        
        source_key = config["source_key"]
        date_column = config.get("date_column")
        transpose = config.get("transpose", False)
        fields = config["fields"]
        
        # æ”¶é›†æ‰€æœ‰å…¬å¸è©²é¡åˆ¥çš„è³‡æ–™
        category_data = {}
        for ticker, data in all_data.items():
            raw = data.get(source_key)
            if not raw:
                continue
            
            try:
                # è§£æ JSON string â†’ DataFrame
                if isinstance(raw, str):
                    df = pd.read_json(StringIO(raw), orient='split')
                else:
                    df = pd.DataFrame(raw)
                
                # è™•ç†è½‰ç½® (è²¡å ±è³‡æ–™: row=ç§‘ç›®, col=æ—¥æœŸ â†’ row=æ—¥æœŸ, col=ç§‘ç›®)
                if transpose:
                    # è²¡å ±è³‡æ–™ç‰¹æ®Šè™•ç†ï¼šcolumns å¯èƒ½æœ‰é‡è¤‡ (2025-09-01, 2025-09-01.1, ...)
                    # éœ€è¦å»é™¤é‡è¤‡ï¼Œåªä¿ç•™ç¬¬ä¸€å€‹ (é€šå¸¸æ˜¯æœ€æ–°/æ­£ç¢ºçš„)
                    
                    # æ¸…ç†æ¬„ä½åç¨±ï¼Œå–å¾—å”¯ä¸€æ—¥æœŸ
                    clean_cols = []
                    seen_dates = set()
                    for col in df.columns:
                        # ç§»é™¤ .1, .2 ç­‰å¾Œç¶´
                        base_date = str(col).split('.')[0]
                        if base_date not in seen_dates:
                            clean_cols.append(col)
                            seen_dates.add(base_date)
                    
                    # åªä¿ç•™å”¯ä¸€æ—¥æœŸçš„æ¬„ä½
                    df = df[clean_cols]
                    
                    # é‡æ–°å‘½åæ¬„ä½ç‚ºä¹¾æ·¨çš„æ—¥æœŸ
                    df.columns = [str(c).split('.')[0] for c in df.columns]
                    
                    # è½‰ç½®
                    df = df.T
                    
                    # è¨­å®šæ—¥æœŸç´¢å¼•
                    df.index = pd.to_datetime(df.index)
                    df = df.sort_index()
                
                # è¨­å®šæ—¥æœŸç´¢å¼• (éè½‰ç½®çš„æƒ…æ³)
                if date_column and date_column in df.columns:
                    df[date_column] = pd.to_datetime(df[date_column])
                    
                    # è™•ç†æ—¥æœŸé‡è¤‡çš„æƒ…æ³: åªä¿ç•™æ¯å€‹æ—¥æœŸçš„ç¬¬ä¸€ç­†
                    if df[date_column].duplicated().any():
                        df = df.drop_duplicates(subset=[date_column], keep='first')
                    
                    df.set_index(date_column, inplace=True)
                    df = df.sort_index()  # ç¢ºä¿æ™‚é–“é †åº
                elif not transpose:
                    # Price è³‡æ–™çš„ index å¯èƒ½å·²ç¶“æ˜¯æ—¥æœŸ
                    if df.index.dtype == 'object' or 'datetime' in str(df.index.dtype):
                        df.index = pd.to_datetime(df.index)
                        df = df.sort_index()
                
                category_data[ticker] = df
                
            except Exception as e:
                # éœé»˜è·³éè§£æå¤±æ•—çš„è³‡æ–™
                continue
        
        if not category_data:
            print(f"      âš ï¸ ç„¡æœ‰æ•ˆè³‡æ–™")
            return
        
        # å°æ¯å€‹æ¬„ä½å»ºç«‹ wide-format DataFrame
        for field_name, field_config in fields.items():
            col_name = field_config["column"]
            desc = field_config["description"]
            
            try:
                # æ”¶é›†è©²æ¬„ä½æ‰€æœ‰å…¬å¸è³‡æ–™
                series_dict = {}
                for ticker, df in category_data.items():
                    if col_name in df.columns:
                        series_dict[ticker] = df[col_name]
                    elif col_name in df.index:
                        # è²¡å ±è³‡æ–™å¯èƒ½ column å’Œ index äº’æ›
                        series_dict[ticker] = df.loc[col_name]
                
                if not series_dict:
                    continue
                
                # åˆä½µæˆ wide-format (rows=æ—¥æœŸ, cols=è‚¡ç¥¨ä»£ç¢¼)
                wide_df = pd.DataFrame(series_dict)
                wide_df = wide_df.sort_index()
                
                # å„²å­˜
                output_path = self.output_dir / category / f"{field_name}.{OUTPUT_FORMAT}"
                
                if OUTPUT_FORMAT == "parquet":
                    wide_df.to_parquet(output_path)
                else:
                    wide_df.to_csv(output_path)
                
                # è¨˜éŒ„ field map
                self.field_map[field_name] = {
                    "category": category,
                    "source_column": col_name,
                    "description": desc,
                    "shape": list(wide_df.shape),
                    "date_range": [str(wide_df.index.min()), str(wide_df.index.max())],
                    "tickers": len(wide_df.columns),
                }
                
                self.stats["total_fields"] += 1
                print(f"      âœ… {field_name:<20} ({wide_df.shape[0]} rows Ã— {wide_df.shape[1]} cols)")
                
            except Exception as e:
                print(f"      âš ï¸ {field_name}: {e}")
    
    def _save_metadata(self):
        """å„²å­˜ metadata"""
        meta_dir = self.output_dir / "_meta"
        
        # 1. è‚¡ç¥¨æ¸…å–®
        tickers_path = meta_dir / "tickers.json"
        with open(tickers_path, 'w', encoding='utf-8') as f:
            json.dump({
                "tickers": sorted(self.tickers),
                "names": self.ticker_names,
                "count": len(self.tickers),
            }, f, ensure_ascii=False, indent=2)
        print(f"   âœ… tickers.json ({len(self.tickers)} æª”è‚¡ç¥¨)")
        
        # 2. æ¬„ä½å°ç…§è¡¨
        field_map_path = meta_dir / "field_map.json"
        with open(field_map_path, 'w', encoding='utf-8') as f:
            json.dump(self.field_map, f, ensure_ascii=False, indent=2)
        print(f"   âœ… field_map.json ({len(self.field_map)} å€‹æ¬„ä½)")
        
        # 3. å»ºæ§‹è³‡è¨Š
        build_info_path = meta_dir / "build_info.json"
        with open(build_info_path, 'w', encoding='utf-8') as f:
            json.dump({
                "build_time": datetime.now().isoformat(),
                "source_dir": str(self.source_dir),
                "output_format": OUTPUT_FORMAT,
                "stats": self.stats,
            }, f, ensure_ascii=False, indent=2)
        print(f"   âœ… build_info.json")
    
    def _print_summary(self):
        """å°å‡ºæ‘˜è¦"""
        print(f"\nğŸ“Š å»ºæ§‹æ‘˜è¦:")
        print(f"   ä¾†æºæª”æ¡ˆ: {self.stats['total_files']} å®¶å…¬å¸")
        print(f"   æˆåŠŸè¼‰å…¥: {self.stats['success_files']} å®¶")
        print(f"   è¼‰å…¥å¤±æ•—: {self.stats['failed_files']} å®¶")
        print(f"   ç”¢å‡ºæ¬„ä½: {self.stats['total_fields']} å€‹")
        print(f"   å»ºæ§‹æ™‚é–“: {self.stats['build_time']}")
        print(f"\nğŸ“ è¼¸å‡ºç›®éŒ„: {self.output_dir}")
        
        # åˆ—å‡ºæ‰€æœ‰æ¬„ä½
        print(f"\nğŸ“‹ æ¬„ä½æ¸…å–®:")
        for category in FIELD_DEFINITIONS.keys():
            category_fields = [f for f, info in self.field_map.items() if info['category'] == category]
            if category_fields:
                print(f"   {category}/")
                for field in category_fields:
                    info = self.field_map[field]
                    print(f"      â”œâ”€â”€ {field}.{OUTPUT_FORMAT} ({info['description']})")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¾¿æ·è®€å–é¡åˆ¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class FieldDB:
    """
    æ¬„ä½è³‡æ–™åº«è®€å–å™¨
    
    ä½¿ç”¨æ–¹å¼:
    >>> db = FieldDB()
    >>> df = db.get('close')           # å–å¾—æ‰€æœ‰å…¬å¸æ”¶ç›¤åƒ¹
    >>> df = db.get('close', '2330')   # å–å¾—å°ç©é›»æ”¶ç›¤åƒ¹
    >>> df = db.get('tej_gpm')         # å–å¾—æ‰€æœ‰å…¬å¸æ¯›åˆ©ç‡
    """
    
    def __init__(self, db_path: Path = None):
        if db_path is None:
            db_path = OUTPUT_DIR
        self.db_path = Path(db_path)
        
        # è¼‰å…¥ metadata
        self.field_map = self._load_json("_meta/field_map.json")
        self.tickers_info = self._load_json("_meta/tickers.json")
        
        # å¿«å–
        self._cache = {}
    
    def _load_json(self, rel_path: str) -> dict:
        """è¼‰å…¥ JSON æª”æ¡ˆ"""
        path = self.db_path / rel_path
        if path.exists():
            with open(path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    @property
    def fields(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¬„ä½"""
        return list(self.field_map.keys())
    
    @property
    def tickers(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰è‚¡ç¥¨ä»£ç¢¼"""
        return self.tickers_info.get("tickers", [])
    
    def get(self, field: str, ticker: str = None, align: bool = True) -> pd.DataFrame:
        """
        å–å¾—æ¬„ä½è³‡æ–™
        
        Args:
            field: æ¬„ä½åç¨± (å¦‚ 'close', 'tej_gpm', 'qfii_net')
            ticker: è‚¡ç¥¨ä»£ç¢¼ (å¯é¸ï¼Œè‹¥æä¾›å‰‡åªå›å‚³è©²è‚¡ç¥¨)
            align: æ˜¯å¦è‡ªå‹•å°é½Šåˆ°æ—¥å ±æ—¥æœŸ (é è¨­ True)
                   å­£å ±/æœˆå ±è³‡æ–™æœƒè‡ªå‹• reindex ä¸¦ ffill
        
        Returns:
            DataFrame (rows=æ—¥æœŸ, cols=è‚¡ç¥¨ä»£ç¢¼)
        """
        if field not in self.field_map:
            raise ValueError(f"æ¬„ä½ä¸å­˜åœ¨: {field}ã€‚å¯ç”¨æ¬„ä½: {self.fields}")
        
        # æª¢æŸ¥å¿«å– (ç”¨ (field, align) ä½œç‚º key)
        cache_key = (field, align)
        if cache_key in self._cache:
            df = self._cache[cache_key]
        else:
            # è¼‰å…¥è³‡æ–™
            info = self.field_map[field]
            category = info["category"]
            
            file_path = self.db_path / category / f"{field}.{OUTPUT_FORMAT}"
            
            if OUTPUT_FORMAT == "parquet":
                df = pd.read_parquet(file_path)
            else:
                df = pd.read_csv(file_path, index_col=0, parse_dates=True)
            
            # è‡ªå‹•å°é½Š: å¦‚æœä¸æ˜¯ price é¡è³‡æ–™ï¼Œå°é½Šåˆ°æ—¥å ±æ—¥æœŸ
            if align and category != "price":
                df = self._align_to_daily(df)
            
            # å¿«å–
            self._cache[cache_key] = df
        
        # è‹¥æŒ‡å®šè‚¡ç¥¨ä»£ç¢¼
        if ticker:
            if ticker not in df.columns:
                raise ValueError(f"è‚¡ç¥¨ä»£ç¢¼ä¸å­˜åœ¨: {ticker}")
            return df[[ticker]]
        
        return df
    
    def _align_to_daily(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        å°‡éæ—¥å ±è³‡æ–™å°é½Šåˆ°æ—¥å ±æ—¥æœŸ
        
        Args:
            df: åŸå§‹è³‡æ–™ (å¯èƒ½æ˜¯å­£å ±ã€æœˆå ±ã€ç±Œç¢¼ç­‰)
        
        Returns:
            å°é½Šåˆ°æ—¥å ±æ—¥æœŸçš„è³‡æ–™ï¼Œç”¨å‰å€¼å¡«å……
        """
        # å–å¾—æ—¥å ±æ—¥æœŸç´¢å¼• (ç”¨ close)
        if 'close' not in self._cache.get(('close', True), {}) if isinstance(self._cache.get(('close', True)), dict) else False:
            close_path = self.db_path / "price" / f"close.{OUTPUT_FORMAT}"
            if OUTPUT_FORMAT == "parquet":
                daily_index = pd.read_parquet(close_path).index
            else:
                daily_index = pd.read_csv(close_path, index_col=0, parse_dates=True).index
        else:
            daily_index = self._cache[('close', True)].index
        
        # å°é½Šä¸¦å¡«å……
        df_aligned = df.reindex(daily_index).ffill()
        
        return df_aligned
    
    def info(self, field: str = None) -> dict:
        """å–å¾—æ¬„ä½è³‡è¨Š"""
        if field:
            return self.field_map.get(field, {})
        return self.field_map
    
    def describe(self):
        """å°å‡ºè³‡æ–™åº«æ‘˜è¦"""
        print("=" * 60)
        print("ğŸ“Š Field Database")
        print("=" * 60)
        print(f"è·¯å¾‘: {self.db_path}")
        print(f"è‚¡ç¥¨æ•¸: {len(self.tickers)}")
        print(f"æ¬„ä½æ•¸: {len(self.fields)}")
        print("\nå¯ç”¨æ¬„ä½:")
        
        by_category = {}
        for field, info in self.field_map.items():
            cat = info['category']
            if cat not in by_category:
                by_category[cat] = []
            by_category[cat].append((field, info['description']))
        
        for cat, fields in by_category.items():
            print(f"\n  {cat}/")
            for field, desc in fields:
                print(f"    â€¢ {field:<20} - {desc}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¸»ç¨‹å¼å…¥å£
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    """ä¸»ç¨‹å¼"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Field Database Builder - æ¬„ä½è³‡æ–™åº«å»ºæ§‹å™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
  python build_field_database.py              # å»ºæ§‹è³‡æ–™åº«
  python build_field_database.py --format csv # ä½¿ç”¨ CSV æ ¼å¼
  python build_field_database.py --list       # åˆ—å‡ºå·²å»ºæ§‹çš„æ¬„ä½
        """
    )
    parser.add_argument('--format', choices=['parquet', 'csv'], default='parquet',
                        help='è¼¸å‡ºæ ¼å¼ (é è¨­: parquet)')
    parser.add_argument('--source', type=str, default=None,
                        help='ä¾†æºç›®éŒ„ (é è¨­: Stock_Pool/Database)')
    parser.add_argument('--output', type=str, default=None,
                        help='è¼¸å‡ºç›®éŒ„ (é è¨­: Platform/FieldDB)')
    parser.add_argument('--list', action='store_true',
                        help='åˆ—å‡ºå·²å»ºæ§‹çš„æ¬„ä½')
    
    args = parser.parse_args()
    
    # ä¿®æ”¹å…¨åŸŸè¨­å®š
    global OUTPUT_FORMAT
    OUTPUT_FORMAT = args.format
    
    if args.list:
        # åˆ—å‡ºå·²å»ºæ§‹çš„æ¬„ä½
        db = FieldDB()
        db.describe()
        return
    
    # å»ºæ§‹è³‡æ–™åº«
    source = Path(args.source) if args.source else SOURCE_DIR
    output = Path(args.output) if args.output else OUTPUT_DIR
    
    builder = FieldDatabaseBuilder(source, output)
    builder.build()
    
    print("\n" + "=" * 70)
    print("ğŸ‰ ä½¿ç”¨æ–¹å¼:")
    print("=" * 70)
    print("""
>>> from Platform.Core.build_field_database import FieldDB
>>> db = FieldDB()
>>> 
>>> # å–å¾—æ‰€æœ‰å…¬å¸æ”¶ç›¤åƒ¹
>>> close = db.get('close')
>>> 
>>> # å–å¾—å°ç©é›»æ”¶ç›¤åƒ¹
>>> tsmc_close = db.get('close', '2330')
>>> 
>>> # å–å¾—æ‰€æœ‰å…¬å¸æ¯›åˆ©ç‡
>>> gpm = db.get('tej_gpm')
>>> 
>>> # åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¬„ä½
>>> print(db.fields)
>>> 
>>> # æŸ¥çœ‹æ¬„ä½è³‡è¨Š
>>> db.describe()
""")


if __name__ == "__main__":
    main()
